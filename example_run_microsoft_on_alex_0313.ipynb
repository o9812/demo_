{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./Recommenders/\")\n",
    "# chagne the working directory to https://www.johnny-lin.com/cdat_tips/tips_pylang/path.html\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "\n",
    "from reco_utils.common.spark_utils import start_or_get_spark\n",
    "from reco_utils.evaluation.spark_evaluation import SparkRankingEvaluation, SparkRatingEvaluation\n",
    "from reco_utils.recommender.sar.sar_singlenode import SARSingleNode\n",
    "from reco_utils.dataset.url_utils import maybe_download\n",
    "from reco_utils.dataset.python_splitters import python_random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.6.8 |Anaconda, Inc.| (default, Dec 29 2018, 19:04:46) \n",
      "[GCC 4.2.1 Compatible Clang 4.0.1 (tags/RELEASE_401/final)]\n",
      "Pandas version: 0.24.1\n",
      "PySpark version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"Pandas version: {}\".format(pd.__version__))\n",
    "print(\"PySpark version: {}\".format(pyspark.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true = pd.read_csv('./fl_processed_test_split_03062019.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_true.columns = ['uid','iid','Rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result from Mars and Alex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yupinglin/anaconda3/envs/reco_pyspark/lib/python3.6/site-packages/numpy/lib/arraysetops.py:522: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "result = pd.read_csv('./result_0313.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>iid</th>\n",
       "      <th>r_ui</th>\n",
       "      <th>est</th>\n",
       "      <th>details</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3524</td>\n",
       "      <td>1672.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>{'was_impossible': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1153</td>\n",
       "      <td>1671.0</td>\n",
       "      <td>3.842258</td>\n",
       "      <td>{'was_impossible': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3992</td>\n",
       "      <td>1670.0</td>\n",
       "      <td>4.688061</td>\n",
       "      <td>{'was_impossible': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3927</td>\n",
       "      <td>1669.0</td>\n",
       "      <td>4.155132</td>\n",
       "      <td>{'was_impossible': False}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3697</td>\n",
       "      <td>1668.0</td>\n",
       "      <td>3.761152</td>\n",
       "      <td>{'was_impossible': False}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   uid   iid    r_ui       est                    details\n",
       "0    0  3524  1672.0  5.000000  {'was_impossible': False}\n",
       "1    0  1153  1671.0  3.842258  {'was_impossible': False}\n",
       "2    0  3992  1670.0  4.688061  {'was_impossible': False}\n",
       "3    0  3927  1669.0  4.155132  {'was_impossible': False}\n",
       "4    0  3697  1668.0  3.761152  {'was_impossible': False}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_mars = result[['uid','iid','r_ui']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred_mne = result[['uid','iid','est']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_USER = 'UserId'\n",
    "COL_ITEM = 'MovieId'\n",
    "COL_RATING = 'Rating'\n",
    "COL_PREDICTION = 'r_ui' \n",
    "\n",
    "HEADER = {\n",
    "    \"col_user\": COL_USER,\n",
    "    \"col_item\": COL_ITEM,\n",
    "    \"col_rating\": COL_RATING,\n",
    "    \"col_prediction\": COL_PREDICTION,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pred df\n",
    " - turn into index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pred = pd.read_csv('./result_0307.csv', index_col=0)\n",
    "# df_pred = pd.read_csv('/Users/yupinglin/Desktop/ForwardLane/ALEX-ranking/data/clean_ranking_result_0307.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank_score</th>\n",
       "      <th>client_id</th>\n",
       "      <th>document_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>ATLSAC6A247DFCFD</td>\n",
       "      <td>Discovery Fund Commentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>HS00000004030108</td>\n",
       "      <td>Discovery Fund Commentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>ATLS5387E9B2267C</td>\n",
       "      <td>Discovery Fund Commentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>ATLS5387E9B2267C</td>\n",
       "      <td>Discovery Fund Commentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>HS00000001395771</td>\n",
       "      <td>Discovery Fund Commentary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank_score         client_id                document_id\n",
       "0         2.0  ATLSAC6A247DFCFD  Discovery Fund Commentary\n",
       "1         2.0  HS00000004030108  Discovery Fund Commentary\n",
       "2         2.0  ATLS5387E9B2267C  Discovery Fund Commentary\n",
       "3         4.0  ATLS5387E9B2267C  Discovery Fund Commentary\n",
       "4         2.0  HS00000001395771  Discovery Fund Commentary"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### true df\n",
    " - turn into index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_true = pd.read_csv('./fl_processed_test_split_03062019.csv', index_col=0)\n",
    "# df_true = pd.read_csv('/Users/yupinglin/Desktop/ForwardLane/ALEX-ranking/data/ranking_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rating matric\n",
    "-  the is `nan` since the prediciton is never show up in the ground true table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = start_or_get_spark(\"EvaluationTesting\", \"local\")\n",
    "\n",
    "dfs_true = spark.createDataFrame(df_true)\n",
    "dfs_pred = spark.createDataFrame(df_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_rate_eval = SparkRatingEvaluation(dfs_true, dfs_pred, **HEADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_rate_eval.rmse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE is nan\n",
      "The R2 is nan\n",
      "The MAE is nan\n",
      "The explained variance is nan\n"
     ]
    }
   ],
   "source": [
    "print(\"The RMSE is {}\".format(spark_rate_eval.rmse()))\n",
    "print(\"The R2 is {}\".format(spark_rate_eval.rsquared()))\n",
    "print(\"The MAE is {}\".format(spark_rate_eval.mae()))\n",
    "print(\"The explained variance is {}\".format(spark_rate_eval.exp_var()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_rank_eval = SparkRankingEvaluation(dfs_true, dfs_pred, k=50, relevancy_method=\"top_k\", **HEADER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top k: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ranking for mars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_USER = 'uid'\n",
    "COL_ITEM = 'iid'\n",
    "COL_RATING = 'Rating'\n",
    "COL_PREDICTION = 'r_ui' \n",
    "\n",
    "HEADER = {\n",
    "    \"col_user\": COL_USER,\n",
    "    \"col_item\": COL_ITEM,\n",
    "    \"col_rating\": COL_RATING,\n",
    "    \"col_prediction\": COL_PREDICTION,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = start_or_get_spark(\"EvaluationTesting\", \"local\")\n",
    "\n",
    "dfs_true = spark.createDataFrame(df_true)\n",
    "dfs_pred = spark.createDataFrame(df_pred_mars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_rank_eval = SparkRankingEvaluation(dfs_true, dfs_pred, k=10, relevancy_method=\"top_k\", **HEADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision at k is 0.23778841967784062\n",
      "The recall at k is 0.3746475734394758\n",
      "The ndcg at k is 0.338815815750349\n",
      "The map at k is 0.20963242015311745\n"
     ]
    }
   ],
   "source": [
    "print(\"The precision at k is {}\".format(spark_rank_eval.precision_at_k()))\n",
    "print(\"The recall at k is {}\".format(spark_rank_eval.recall_at_k()))\n",
    "print(\"The ndcg at k is {}\".format(spark_rank_eval.ndcg_at_k()))\n",
    "print(\"The map at k is {}\".format(spark_rank_eval.map_at_k()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_rank_eval = SparkRankingEvaluation(dfs_true, dfs_pred, k=5, relevancy_method=\"top_k\", **HEADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision at k is 0.28062690465825\n",
      "The recall at k is 0.22392751760405213\n",
      "The ndcg at k is 0.29156365754124514\n",
      "The map at k is 0.15165091688491777\n"
     ]
    }
   ],
   "source": [
    "print(\"The precision at k is {}\".format(spark_rank_eval.precision_at_k()))\n",
    "print(\"The recall at k is {}\".format(spark_rank_eval.recall_at_k()))\n",
    "print(\"The ndcg at k is {}\".format(spark_rank_eval.ndcg_at_k()))\n",
    "print(\"The map at k is {}\".format(spark_rank_eval.map_at_k()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_rank_eval = SparkRankingEvaluation(dfs_true, dfs_pred, k=20, relevancy_method=\"top_k\", **HEADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision at k is 0.1712233347845015\n",
      "The recall at k is 0.5268346912121399\n",
      "The ndcg at k is 0.41172638220397156\n",
      "The map at k is 0.24687015498401804\n"
     ]
    }
   ],
   "source": [
    "print(\"The precision at k is {}\".format(spark_rank_eval.precision_at_k()))\n",
    "print(\"The recall at k is {}\".format(spark_rank_eval.recall_at_k()))\n",
    "print(\"The ndcg at k is {}\".format(spark_rank_eval.ndcg_at_k()))\n",
    "print(\"The map at k is {}\".format(spark_rank_eval.map_at_k()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ranking for NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_USER = 'uid'\n",
    "COL_ITEM = 'iid'\n",
    "COL_RATING = 'Rating'\n",
    "COL_PREDICTION = 'est' \n",
    "\n",
    "HEADER = {\n",
    "    \"col_user\": COL_USER,\n",
    "    \"col_item\": COL_ITEM,\n",
    "    \"col_rating\": COL_RATING,\n",
    "    \"col_prediction\": COL_PREDICTION,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = start_or_get_spark(\"EvaluationTesting\", \"local\")\n",
    "dfs_pred_mne = spark.createDataFrame(df_pred_mne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_rank_eval_mne = SparkRankingEvaluation(dfs_true, dfs_pred_mne, k=10, relevancy_method=\"top_k\", **HEADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision at k is 0.13217239878101875\n",
      "The recall at k is 0.2004153105154411\n",
      "The ndcg at k is 0.20983389233741537\n",
      "The map at k is 0.1173076550191498\n"
     ]
    }
   ],
   "source": [
    "print(\"The precision at k is {}\".format(spark_rank_eval_mne.precision_at_k()))\n",
    "print(\"The recall at k is {}\".format(spark_rank_eval_mne.recall_at_k()))\n",
    "print(\"The ndcg at k is {}\".format(spark_rank_eval_mne.ndcg_at_k()))\n",
    "print(\"The map at k is {}\".format(spark_rank_eval_mne.map_at_k()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_rank_eval_mne = SparkRankingEvaluation(dfs_true, dfs_pred_mne, k=5, relevancy_method=\"top_k\", **HEADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision at k is 0.1887679582063561\n",
      "The recall at k is 0.14455587343049256\n",
      "The ndcg at k is 0.20976641591907902\n",
      "The map at k is 0.09943582957296496\n"
     ]
    }
   ],
   "source": [
    "print(\"The precision at k is {}\".format(spark_rank_eval_mne.precision_at_k()))\n",
    "print(\"The recall at k is {}\".format(spark_rank_eval_mne.recall_at_k()))\n",
    "print(\"The ndcg at k is {}\".format(spark_rank_eval_mne.ndcg_at_k()))\n",
    "print(\"The map at k is {}\".format(spark_rank_eval_mne.map_at_k()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_rank_eval_mne = SparkRankingEvaluation(dfs_true, dfs_pred_mne, k=20, relevancy_method=\"top_k\", **HEADER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The precision at k is 0.08354375272094036\n",
      "The recall at k is 0.2522474103558126\n",
      "The ndcg at k is 0.23443931404597732\n",
      "The map at k is 0.12670985953617006\n"
     ]
    }
   ],
   "source": [
    "print(\"The precision at k is {}\".format(spark_rank_eval_mne.precision_at_k()))\n",
    "print(\"The recall at k is {}\".format(spark_rank_eval_mne.recall_at_k()))\n",
    "print(\"The ndcg at k is {}\".format(spark_rank_eval_mne.ndcg_at_k()))\n",
    "print(\"The map at k is {}\".format(spark_rank_eval_mne.map_at_k()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (reco_pyspark)",
   "language": "python",
   "name": "my_env_name"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
